{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрика качества BLEU  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU - Bilingual Evaluation Understudy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks.\n",
    "\n",
    "The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence.\n",
    "\n",
    "A perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.\n",
    "\n",
    "The score was developed for evaluating the predictions made by automatic machine translation systems. It is not perfect, but does offer 5 compelling benefits:\n",
    "   * It is quick and inexpensive to calculate;\n",
    "   * It is easy to understand;\n",
    "   * It is language independent;\n",
    "   * It correlates highly with human evaluation;\n",
    "   * It has been widely adopted.\n",
    "   \n",
    "The BLEU score was proposed by Kishore Papineni, et al. in their 2002 paper “BLEU: a Method for Automatic Evaluation of Machine Translation“.\n",
    "\n",
    "How it works:\n",
    "The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is.\n",
    "\n",
    "In addition to translation, we can use the BLEU score for other language generation problems with deep learning methods such as:\n",
    "   * Language generation;\n",
    "   * Image caption generation;\n",
    "   * Text summarization;\n",
    "   * Speech recognition.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence BLEU Score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides the sentence_bleu() function for evaluating a candidate sentence against one or more reference sentences.\n",
    "\n",
    "The reference sentences must be provided as a list of sentences where each reference is a list of tokens. The candidate sentence is provided as a list of tokens. \n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'old', 'man', 'walking', 'with', 'girl'], ['man', 'and', 'girl', 'are','walking', 'together']]\n",
    "candidate = ['the', 'old', 'man', 'walking', 'with', 'girl']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also provides a function called corpus_bleu() for calculating the BLEU score for multiple sentences such as a paragraph or a document.\n",
    "\n",
    "The references must be specified as a list of documents where each document is a list of references and each alternative reference is a list of tokens, e.g. a list of lists of lists of tokens. The candidate documents must be specified as a list where each document is a list of tokens, e.g. a list of lists of tokens.\n",
    "\n",
    "This is a little confusing; here is an example of two references for one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# two references for one document\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "reference = [[['the', 'old', 'man', 'walking', 'with', 'girl'], ['man', 'and', 'girl', 'are','walking', 'together']]]\n",
    "candidate = [['the', 'old', 'man', 'walking', 'with', 'girl']]\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that in our situation Sentence BLEU Score is more suitable. So in further examples I will use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative and Individual BLEU Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU score calculations in NLTK allow you to specify the weighting of different n-grams in the calculation of the BLEU score.\n",
    "\n",
    "This gives you the flexibility to calculate different types of BLEU score, such as individual and cumulative n-gram scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Individual N-Gram Scores ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 слов совпадает, одно нет => BLUE = 5/6 = 0.833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'old', 'man', 'walking', 'with', 'girl'], ['man', 'and', 'girl', 'are','walking', 'together']]\n",
    "candidate = ['the', 'old', 'man', 'walking', 'with', 'boy']\n",
    "score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this example for individual n-grams from 1 to 4 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.833333\n",
      "Individual 2-gram: 0.800000\n",
      "Individual 3-gram: 0.750000\n",
      "Individual 4-gram: 0.666667\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'old', 'man', 'walking', 'with', 'girl'], ['man', 'and', 'girl', 'are','walking', 'together']]\n",
    "candidate = ['the', 'old', 'man', 'walking', 'with', 'boy']\n",
    "\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can calculate the individual BLEU scores, this is not how the method was intended to be used and the scores do not carry a lot of meaning, or seem that interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Cumulative N-Gram Scores ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cumulative scores refer to the calculation of individual n-gram scores at all orders from 1 to n and weighting them by calculating the weighted geometric mean.\n",
    "\n",
    "By default, the sentence_bleu() and corpus_bleu() scores calculate the cumulative 4-gram BLEU score, also called BLEU-4.\n",
    "\n",
    "The weights for the BLEU-4 are 1/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and 4-gram scores. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.833333\n",
      "Cumulative 2-gram: 0.816497\n",
      "Cumulative 3-gram: 0.795536\n",
      "Cumulative 4-gram: 0.759836\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'old', 'man', 'walking', 'with', 'girl'], ['man', 'and', 'girl', 'are','walking', 'together']]\n",
    "candidate = ['the', 'old', 'man', 'walking', 'with', 'boy']\n",
    "\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** One more example *** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.777778\n",
      "Individual 2-gram: 0.500000\n",
      "Individual 3-gram: 0.428571\n",
      "Individual 4-gram: 0.333333\n",
      "Cumulative 1-gram: 0.777778\n",
      "Cumulative 2-gram: 0.623610\n",
      "Cumulative 3-gram: 0.553618\n",
      "Cumulative 4-gram: 0.485492\n"
     ]
    }
   ],
   "source": [
    "# two word different\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'brown', 'dog', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "#score = sentence_bleu(reference, candidate)\n",
    "#print(score)\n",
    "\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))\n",
    "\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.661873\n",
      "Individual 2-gram: 0.252142\n",
      "Individual 3-gram: 0.147083\n",
      "Individual 4-gram: 0.000000\n",
      "Cumulative 1-gram: 0.661873\n",
      "Cumulative 2-gram: 0.408517\n",
      "Cumulative 3-gram: 0.293867\n",
      "Cumulative 4-gram: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/P_Moiseenkov/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'brown', 'dog', 'jumped',  'over', 'the', 'dog']\n",
    "#score = sentence_bleu(reference, candidate)\n",
    "#print(score)\n",
    "\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))\n",
    "\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** ------------------------------------------- ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** In our problem we will apply sentence_bleu(), cumulative 4-gram BLEU score, also called BLEU-4 ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** ------------------------------------------- ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
